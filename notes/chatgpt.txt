https://zhuanlan.zhihu.com/p/589533490
GPT	2018年6月	1.17 亿	约 5GB
GPT-2	2019年2月	15 亿	40GB
GPT-3	2020年5月	1,750 亿	45TB，其中GPT-3训练一次的费用是460万美元，总训练成本达1200万美元
ChatGPT 来自 OpenAI 研究实验室，由 GPT-3.5 系列模型提供支持，包括 3.5 之前的模型版本，都使用 Azure AI 超级计算基础结构上的文本和代码数据进行训练
据NVIDIA估算，如果要训练GPT-3，即使单个机器的显存/内存能装得下，用8张V100的显卡，训练时长预计要36年；即使用512张V100 ，训练也需要将近7个月；如果拥有1024张80GBA100， 那么完整训练GPT-3 的时长可以缩减到1个月

chatGPT在效果强大的GPT 3.5大规模语言模型（LLM，Large Language Model）基础上，
引入“人工标注数据+强化学习”（RLHF，Reinforcement Learning from Human Feedback ，这里的人工反馈其实就是人工标注数据）来不断Fine-tune预训练语言模型，
在“人工标注数据+强化学习”框架下，具体而言，ChatGPT的训练过程分为以下三个阶段:
1.prompt+高质量标注数据，即<prompt,answer>数据来Fine-tune GPT 3.5模型
2.训练回报模型(Reward Model,RM),对第一步的模型，同一个问题生成的k个结果进行人工标注，用ltr模型训练回报模型，给k个结果打分，
输入<prompt,answer>，输出结果的质量得分，得分越高说明产生的回答质量越高。
3.采用强化学习来增强预训练模型的能力,回报模型的得分就是强化学习的reward
缺点：
1.有的答案不准确
2.更新新知识问题，如果从新训练gpt成本高，只进行fine-tune的话会有知识遗忘
3.实际应用成本较高，
问题1和2可以引入传统搜索引擎解决，传统索索引擎听过知识验证的相关信息展示，新的知识可以用传统搜索引擎进行回答

初代GPT-3模型通过预训练获得生成能力、世界知识和in-context learning。
然后通过instruction tuning的模型分支获得了遵循指令和能泛化到没有见过的任务的能力。
经过代码训练的分支模型则获得了代码理解的能力，作为代码训练的副产品，模型同时潜在地获得了复杂推理的能力。
结合这两个分支，code-davinci-002似乎是具有所有强大能力的最强GPT-3.5模型。
接下来通过有监督的instruction tuning和 RLHF通过牺牲模型能力换取与人类对齐，即对齐税。
RLHF 使模型能够生成更翔实和公正的答案，同时拒绝其知识范围之外的问题。