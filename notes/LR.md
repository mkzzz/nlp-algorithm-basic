 基础知识
1.LR
介绍一下？公式推导？损失函数及其图像？与线性回归的关系？你在什么地方用过？怎么用的？
Logistic回归，逻辑回归，是在线性回归的基础上加上一个sigmod函数，损失函数是log损失函数， ,简单的二分类任务
推倒过程
1.	Sigmod函数
 
2.	假设每个正样本的概率是
每个样本和负样本的概率可以写成
                                                                                                                                                                                                                                                                                                                                                                                                                                                  
则，根据极大似然函数，m个样本的概率如下，目标是使这m个样本的概率最大
 
因为似然函数是指数形式，且越大越好，则对这个似然函数进行
1.	取对数
2.	加上负号（得到loss函数，越小越好）
 
对loss的求导，
注意y是常数
 
 
对loss求导后，得到
 , 

Softmax
 
损失函数
 
对loss求导后，得到：
 

全连接层和softmax的关系
全连接是展开项，吧神经网络层提取的体征维度进行变换，先拼接展开，再变换，得到想要分类的维度，再用softmax进行归一化
为什么用sigmod和softmax进行归一化?
们现在关心的是，给定某些假设之后，熵最大的分布。也就是说这个分布应该在满足我假设的前提下越均匀越好。比如大家熟知的正态分布，正是假设已知mean和variance后熵最大的分布。
回过来看logistic regression，这里假设了什么呢？首先，我们在建模预测 Y|X，并认为 Y|X 服从bernoulli distribution，所以我们只需要知道 P(Y|X)；其次我们需要一个线性模型，所以 P(Y|X) = f(wx)。接下来我们就只需要知道 f 是什么就行了。而我们可以通过最大熵原则推出的这个 f，就是sigmoid

2.正则化
解释下正则化？
L1正则和L2正则
L1正则，筛选特征 ，L1可以让一部分特征的系数缩小到0，从而间接实现特征选择。所以L1适用于特征之间有关联的情况
L2正则， ，防止θ过大。防止过拟合， L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况
 
如上图所示，l1正则更容易产生稀疏解，达到筛选特征的效果

3.DT
ID3？C4.5？CART？区别，优点、缺点，计算公式？
为什么DT不适合稀释数据？
预剪枝？后减枝？
历史回顾：1984年提出的cart，1986年提出的ID3，1993年提出的c4.5
CART一般是GINI系数分支
 
C4.5一般是信息增益率分支
 
ID3是信息增益分支
 
 
CART和C4.5之间主要差异在于分类结果上，CART可以回归分析也可以分类，C4.5只能做分类；C4.5子节点是可以多分的，而CART是无数个二叉子节点；
以此拓展出以CART为基础的“树群”random forest ， 以回归树为基础的“树群”GBDT
ID3和C4.5只能做分类，CART（分类回归树）不仅可以做分类（0/1）还可以做回归（0-1）
ID3和C4.5节点上可以产出多叉（低、中、高），而CART节点上永远是二叉（低、非低）
预剪枝：就是在构建决策树的时候提前停止。比如指定树的深度最大为3，那么训练出来决策树的高度就是3,预剪枝主要是建立某些规则限制决策树的生长，降低了过拟合的风险，降低了建树的时间，但是有可能带来欠拟合问题。
后剪枝：后剪枝是一种全局的优化方法，在决策树构建好之后，然后才开始进行剪枝。后剪枝的过程就是删除一些子树，这个叶子节点的标识类别通过大多数原则来确定，即属于这个叶子节点下大多数样本所属的类别就是该叶子节点的标识。选择减掉哪些子树时，可以计算没有减掉子树之前的误差和减掉子树之后的误差，如果相差不大，可以将子树减掉。一般使用后剪枝得到的结果比较好。
 
决策树是根据特征做的分叉树模型，你的数据是稀疏的情况下，比如某个特征0特别多，然后再做决策的时候很容易就会根据这个特征把数据分开的，容易造成过拟合的，还有就是一般稀疏的数据他的特征就比较多，那么也容易过拟合 

3.1 Adaboost
介绍一下Adaboost？手推Adaboost·
3.2 GBDT
GBDT腾讯面试必问，因为我去的鹅厂现场面试的，被问到了GBDT的相关内容，而旁边一个面试官在电话面试，也是在问GBDT的内容。
介绍下GBDT？GBDT的基模型是什么，是分类树还是回归树？Gradient的体现？为什么要使用多棵树来不断缩小残差？而不是使用一棵拟合得更好的数？GBDT如何构建特征？GBDT如何用于分类，写出公式？
3.3 XGBoost
手推XGBoost必备的。
你在什么地方用到了XGBoost？它和GBDT的区别在哪里？你为什么不用LightGBM？
XGBoost不手推一下，说的内容感觉会很苍白。
3.4 随机森林
解释下随机森林？
你用它做过什么？为什么可以用它？
解释下bagging？

4.SVM
 
 
KKT条件是泛拉格朗日乘子法的一种形式；主要应用在当我们的优化函数存在不等值约束的情况下的一种最优化求解方式；KKT条件即满足不等式约束情况下的条件

手推SVM，必备的。
SVM的损失函数是什么，为什么选择它作为损失函数？
为什么用对偶？
什么是对偶问题
任何一个求极大化的线性规划问题都有一个求极小化的线性规划问题与之对应，反之亦然，如果我们把其中一个叫原问题，则另一个就叫做它的对偶问题
核函数是什么，有什么作用？你用过哪几种核函数？这里最好写出来。
损失函数
 
等价于
 
第一个是支持向量到超平面的距离1/w，越大越好，
第二个是，样本点落在分割平面的两侧
 
KKT条件的理解

 
 
看上述推导过程得知，满足此时的对偶问题的kkt条件是
主要是第二个条件是kkt推到出来的，第一个是求极值时导数等于0, 3和4和5是初始条件 
 
 
Svm求解：
 
 
 
对偶问题
转化成等价的对偶问题，为了求解方便
min max的形式来得到最优解。而这种写成这种形式对x不能求导，所以我们需要转换成max min的形式，这时候，x就在里面了，这样就能对x求导了。而为了满足这种对偶变换成立，就需要满足KKT条件（KKT条件是原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件）。
核函数的作用
在低维空间上的计算量等价于特征做维度扩展后的点乘的结果。
 
高斯核（重点）
 
 
5.深度学习部分
解释一下交叉熵、相对熵，它们的关系？互信息呢？
Cross Entropy是什么，它和log loss的区别？
Dropout防止过拟合的原理？
CNN如何用于NLP？CNN和RNN在NLP中能到达什么效果，怎么选择用哪一个？
梯度消失？梯度爆炸？
解释下LSTM，画出结构？GRU？为什么用LSTM不用RNN？
信息熵(information entropy)
 
条件熵(Conditional entropy)
 
相对熵 (Relative entropy)，也称KL散度 (Kullback–Leibler divergence)

 
总结：相对熵可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 pp 与 qq之间的对数差在 pp 上的期望值
交叉熵 (Cross entropy)
 
交叉熵可以用来计算学习模型分布与训练分布之间的差异。交叉熵广泛用于逻辑回归的Sigmoid和Softmax函数中作为损失函数使用
相对熵和交叉熵的关系
 
互信息
互信息，Mutual Information缩写为MI，表示两个变量X与Y是否有关系，以及关系的强弱
 
Dropout防止过拟合的原理
防止参数过分依赖训练数据，增加参数对数据集的泛化能力
dropout防止参数过分依赖训练数据，在每次训练的时候，每个神经元有一定的几率被移除，这样可以让一个神经元的出现不应该依赖于另外一个神经元，增加参数对数据集的泛化能力
CNN如何用于NLP？CNN和RNN在NLP中能到达什么效果，怎么选择用哪一个？
Textcnn
最天然适合于CNN的应该是分类任务，比如情感分析(Sentiment Analysis)，垃圾检测(Spam Detection)和主题分类(Topic Categorization)。
以下图中 ，每个颜色的代表一个卷积核，多少行代表一次卷积的词数，也就相当于n-gram的关系Convolutional neural networks for sentence classification
 
梯度消失？梯度爆炸？
链式求，对激活函数进行求导
此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生梯度爆炸，
如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失
解释下LSTM，画出结构？GRU？为什么用LSTM不用RNN？
lstm
 
gru
 
Lstm可以有效减缓梯度消失
 
5.1 lstm
https://blog.csdn.net/wangyangzhizhou/article/details/76651116
Long Short-Term Memory
 
遗忘门
 
输入门
 
 
输出门
 

Lstm和gru的区别
6.特征工程
如何处理缺失值？异常值？为什么这么处理？
类别型数据的处理？连续性数据的处理？你怎么分桶的？
数据不平衡怎么做？要注意什么？
缺失值：
1，删除
2，补齐
平均值：数值平均值，非数值，填出现频率最高的值
3，不处理
异常值：
1，删除
2，当做缺失值处理
3，不处理
分桶可以对不同长度的句子进行训练啊，提高效果，我们训练的时候是对句子有个最大长度的限制，如果句子没有那么长的就padding0的，如果都是按照所以分桶可以对不同长度的句子进行处理，避免所有的（包括短句）也padding太多的0
数据不平衡：
正负样本比例差别较大，
1，采样，大众样本下采样，小众样本正采样
2，数据合成
3，加权，不同类别分错的代价不同
4，1分类（异常值检测）

7.聚类算法 
你用过什么聚类算法？解释下原理？
K-means有什么问题？如何解决这些问题？
说一说“中餐馆问题”——抱歉，这个我现在还没搞明白。
Kmeans
选择初始化的k个类别中心a1,a2,...ak;
对于每个样本Xi，将其标记位距离类别中心aj最近的类别j
更新每个类别的中心点aj为隶属该类别的所有样本的均值
重复上面两步操作，直到达到某个中止条件
迭代次数、最小平方误差MSE(样本到中心的距离平方和)、簇中心点变化率
距离计算：
曼哈顿距离，a-b加上b-c
欧氏距离a-c的平方开根号
夹角余弦 a的摸乘以c的摸，再乘以cos（i），i是a和c的夹角
 
K-means算法在迭代的过程中使用所有点的均值作为新的质点(中心点)，如果簇中存在异常点，将导致均值偏差比较严重（可以使用中位数的聚类方式叫做K-Mediods聚类(K中值聚类)）
K-means算法是初值敏感(K值的给定和K个初始簇中心点的选择)的，选择不同的初始值可能导致不同的簇划分规则
缺点：
K值是用户给定的，在进行数据处理前，K值是未知的，不同的K值得到的结果也不一样；
对初始簇中心点是敏感的
不适合发现非凸形状的簇或者大小差别较大的簇
特殊值(离群值)对模型的影响比较大
优点：
理解容易，聚类效果不错
处理大数据集的时候，该算法可以保证较好的伸缩性和高效率
当簇近似高斯分布的时候，效果非常不错
初始值敏感的解决
1.二分K-Means算法，每次选择一个最大的簇，划分成2个簇，重复操作
2.K-Means++算法，每个点x，计算x到所有已有聚类中心点的距离和D(X)，基于D(X)采用线性概率选择出下一个聚类中心点(距离较远的一个点成为新增的一个聚类中心点)
K-Means||算法
3.多选几次
中餐馆问题
 
8.	降维
https://blog.csdn.net/wangjian1204/article/details/50642732
解释下PCA？LDA？SVD？区别？手推。
解释下协同过滤？
怎么降维？遇到的数据最大多少维？你怎么做？为什么这么做？
pca
PCA(Principal Component Analysis)，即主成分分析方法
算法目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并且期望在所投影的维度上数据的方差最大（最大方差理论）
PCA是为了让映射后的样本具有更大的发散性，PCA是无监督的学习算法
PCA是一种丢失原始数据信息最少的无监督线性降维方式
 
对角矩阵(diagonal matrix)是一个主对角线之外的元素皆为0的矩阵
　SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m×n的矩阵，那么我们定义矩阵A的SVD为：
A=UΣVT
　　　　其中U是一个m×m的矩阵，Σ是一个m×n的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个n×n的矩阵。U和V都是酉矩阵，即满足UTU=I,VTV=I。下图可以很形象的看出上面SVD的
对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：
 
简单来说，特征值分解，是相对于方阵来说的，m*m分解成m*m,m*m,m*m,中间的是特征值矩阵，是对角阵，从大到小排列时，减小的很快，所以可以取k个来近似，m*k,k*k,k*m
将k*m乘到等号的左边来，得到m*m，m*k 得到m*k就是pca降维
相对来说，svd分解，就是分解的不是方阵，m*n，其他的道理一样，注意，
左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降
 左奇异矩阵，乘到等号左边来，进行行的压缩
  右奇异矩阵，乘到等号左边来，进行列的压缩
lda
Linear Discriminant Analysis（线性判别分析）
LDA是为了让映射后的样本有最好的分类性能，LDA是有监督学习算法。
LDA是一种基于分类模型进行特征属性合并的操作，是一种有监督的降维方法。
 相同点：
 两者均可以对数据完成降维操作
 两者在降维时候均使用矩阵分解的思想
 两者都假设数据符合高斯分布
 不同点：
 LDA是监督降维算法，PCA是无监督降维算法
 LDA降维最多降到类别数目k-1的维数，而PCA没有限制
 LDA除了降维外，还可以应用于分类
 LDA选择的是分类性能最好的投影，而PCA选择样本点投影具有最大方差的方向
除了使用PCA和LDA降维外，还可以使用主题模型来达到降维的效果
Svd
协同过滤
怎么降维？遇到的数据最大多少维？你怎么做？为什么这么做
特征矩阵过大，导致计算量比较大
 在实际的机器学习项目中，特征选择/降维是必须进行的，因为在数据中存在以下几个方面的问题：
 数据的多重共线性：特征属性之间存在着相互关联关系。多重共线性会导致解的空间不稳定，从而导致模型的泛化能力弱；
 高纬空间样本具有稀疏性，导致模型比较难找到数据特征；
 过多的变量会妨碍模型查找规律；
 仅仅考虑单个变量对于目标属性的影响可能忽略变量之间的潜在关系。
 通过降维的目的是：
 减少特征属性的个数
 确保特征属性之间是相互独立的
9.模型评估
Accuracy、Precision、Recall、F1-score、ROC、AUC
以上，概念、公式？
数据不平衡用什么评估指标？
P-R曲线和ROC的区别？
 
 
准确率(accuracy)
预测的正确样本数/总样本数
精确率(precision)
预测的正确样本中：p = 正确的正例样本数/预测为正例的样本数
召回率(recall)
真实的正确样本中：r = 正确的正例样本数/样本中的正例样本数——覆盖率
精确率又可以称为查准率，召回率又可以称为查全率
F1-score
2(p*r)/(p+r) 值为正确率和召回率的调和平均值
 
这三种平均数各有利弊，但调和平均数受极端值影响较小，更适合评价不平衡数据的分类问题
F1 score选择了第一种调和平均数算法进行计算；该算法的特点就是会更多聚焦在较低的值；所以会对每个指标非常重视
F分数也被广泛应用在自然语言处理领域，比如命名实体识别、分词等，用来衡量算法或系统的性能
是统计学中用来衡量二分类模型精确度的一种指标。它同时兼顾了分类模型的精确率和召回率。F1分数可以看作是模型精确率和召回率的一种加权平均，它的最大值是1，最小值是0
ROC（Receiver Operating Characteristic）
受试者工作特征曲线 又称为感受性曲线（sensitivity curve）
点越接近（0,1）越好，面积越大越好
 
横坐标：FPR，“假正例率” （False Positive Rate 简称FPR），c/(c+d)
纵坐标：TPR，“真正例率”（True Positive Rate 简称TPR）也是召回率
如果二元分类器输出的是对正样本的一个分类概率值，当取不同阈值时会得到不同
的混淆矩阵，对应于ROC曲线上的一个点。那么ROC曲线就反映了FPR与TPR之间权
衡的情况，通俗地来说，即在TPR随着FPR递增的情况下，谁增长得更快，快多少的
问题。TPR增长得越快，曲线越往上屈，AUC就越大，反映了模型的分类性能就越
好。当正负样本不平衡时，这种模型评价方式比起一般的精确度评价方式的好处尤
其显著。
个人理解：
曲线上的点，是对于不同情况下，fpr，tpr的一组取值，这个不同条件是指，预测为正样本和负样本的阈值不同（正常情况是0.5），fpr从小到大也意味着，阈值从0-1变化（例如阈值是0.3，意思是预测结果大于0.3就是负样本，小于0.3是正样本）
阈值越小，也就是fpr越小，也就是说判断为正样本的概率越低，此时tpr也就是召回率，代表了真实值中预测对的概率，如果此值很大，说明在不利于判断正样本的条件下，模型仍然能很准确的判断出正样本，所以是好模型
随着阈值增大，tpr必然会增大，增大较快的模型比增大较慢的模型，能在不利于判断正样本的条件下仍然有好的效果，也就是模型的鲁棒性强，所以是好模型
所以，判断roc曲线下的面积越大越好
AUC（Area Under Curve）
ROC曲线下的面积，越大越好
显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而AUC作为数值可以直观的评价分类器的好坏，值越大越好。
• AUC = 1，是完美分类器
• 0.5 < AUC < 1，优于随机猜测
• AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
• AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测
P-R曲线和ROC的区别
线性回归的模型判别标准
Mse（Mean Squared Error）
 
Rmse（Root Mean Squard Error）
均方根误差
R方
1-误差/方差，越大越好

10.	损失函数
交叉熵损失（分类）
平方误差和损失（回归）
合页损失（svm）
Crf损失（算出p（y），在计算交叉熵损失）

11.	HMM
HMM
隐马尔可夫模型(Hidden Markov Model, HMM)是一种统计模型，在语音识别、行为识别、NLP、故障诊断等领域具有高效的性能
HMM是一个双重随机过程---具有一定状态的隐马尔可夫链和随机的观测序列
 
HMM由隐含状态S、可观测状态O、初始状态概率矩阵/向量π、隐含状态转移概率矩阵A、可观测值转移矩阵B(又称为混淆矩阵，Confusion Matrix)；
 π和A决定了状态序列，B决定观测序列，因此HMM可以使用三元符号表示，称为HMM的三元素：
 
参数说明
 
=============================================================
 
============================================================
 
============================================================
 
============================================================
Hmm的2个性质
 
三个问题
 
学习问题时，怎么求出参数（状态转移阵，发射阵）
1.	如果有大量的标注语料，则可直接统计计算概率即可
2.	如果没有标注语料，则用算法训练确定
Viterbi算法实例
Viterbi算法实际是用动态规划的思路求解HMM预测问题，求出概率最大的“路径”，每条“路径”对应一个状态序列
例子
 
 
 
例子中，按照白黑白白黑的顺序，计算每层从每个盒子中选出的数值 ，最后得到序列
计算中，用到了，A,B,π，三个参数
计算原理
 

12.	LDA
主题模型是对文本中隐含主题的一种建模方法，每个主题其实是词表上单词的概率分布；
 主题模型是一种生成模型，一篇文章中每个词都是通过“以一定概率选择某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到的
Lda是用一个生成模型（生成模型与判别式模型的区别大家还是要懂的）来建模文章的生成过程
 
 
由于可以使用小矩阵来近似的描述样本信息的这个重要特性，SVD可以常用于PCA降维、推荐系统以及主题模型等场景中
 
潜在语义分析(Latent Semantic Analysis, LSA)，也叫做Latent Semantic Indexing, LSI. 是一种常用的简单的主题模型。LSA是基于奇异值分解(SVD)的方法得到文本主题的一种方式
分解后，左边的vt，的列就是文档，行就是主题，然后吧每个文档的向量进行相似度计算
 
lda
隐含狄利克雷分布(Latent Dirichlet Allocation, LDA)是一种基于贝叶斯算法模型，利用先验分布对数据进行似然估计并最终得到后验分布的一种方式。LDA是一种比较常用的主题模型
 
详细
https://www.jianshu.com/p/74ec7d5f6821
 
主题模型
LDA模型可用于主题预测，预测文章中的主题分布，有了主题分布，可以做文本分类
 
求解模型的过程，就是讲训练数据根据词袋法处理后，直接输入到模型接口中，并定义主题个数，那么模型就会训练得到参数
 
 
13.	NB
 
朴素贝叶斯(Naive Bayes， NB)是基于“特征之间是独立的”这一朴素假设，应 
用贝叶斯定理的监督学习算法
推导
 
 
算法流程
 
应用流程
 
14.	n_gram
是一种语言模型
比如n=2，就是统计相邻2个词的出现的频率，进而转化成概率，然后，进行句子的概率计算，得出最大的概率
可以用来分词
实际的存储
 
推导
 
15.	CRF
https://blog.csdn.net/Scythe666/article/details/82021692
条件随机出场，
Crf属于判别式，hmm属于生成式
 
 
条件随机场(conditional random field, CRF)是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫随机场
场景：分词、词性标注、实体识别
优点：精度高、输出序列长度可变
缺点：速度不如HMMs
Crf为什么比HMM在分词上更精准？
CRF在给定了观察序列的情况下，对整个的序列的联合概率有一个统一的指 数模型，可以更好更多的利用待识别文本中所提供的上下文信息以得更好的实验结果
意思是crf考虑的是序列的整体概率，而hmm考虑的只是相邻间的转移概率和从隐藏状态到可观测状态的发射概率

MRF：概率无向图模型(probabilistic undireoted graphical model)，又称为马尔可夫随机场(Markov random field)，是一个可以由无向图表示的联合概率分布。
前提条件：马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。
马尔科夫性：无向图每个节点总只受相连节点的影响。

(条件随机场) 设X与Y是随机变量，P(Y | X)是在给定X的条件下Y的条件概率分布。若随机变量Y构成一个由无向图G=(V,E)表示的马尔可夫随机场，即对任意结点v成立，则称条件概率分布P(Y|X)为条件随机场

线性链条件随机场：定义在线性链上的特殊的条件随机场，称为线性链条件随机场(linear chain conditional random field )。
在条件概率模型P(Y|X)中，Y是输出变量，表示标记序列，也把标记序列称为状态序列，
X是输入变量，表示需要标注的观测序列。
 
学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型；
预测时，对于给定的输入序列x，求出条件概率最大的输出序列。
 
公式推导
 
 
模型学习
有了概率，就是可以用极大似然求解（和lr的推导一样，是交叉熵损失）
 
 
模型预测（维特比）
 
NLP方向的问题
1.TF-IDF
原理、公式？为什么用log？为什么“+1”？缺点？
TF-IDF(Item frequency-inverse document frequency)
TF的意思是词频(Item Frequency)，IDF的意思是逆向文件频率(Inverse Document Frequency)。
TF-IDF可以反映语料中单词对文档/文本的重要程度
 
加1是为了防止分母为0
文档集中含某个词的数量等于总的文档集数量，即N/n=1，取对数能使逆文档率等于0，也就说明了这个不能区别某文与文档中其他文章
 
 

2.NNLM、Word2vec、Doc2vec、
介绍一下，解释一下模型？
怎么选择使用Word2vec、Doc2vec？
解释下Huffman Tree或Negative Sampling？详细说下Huffman Tree或Negative Sampling的过程？
Word2vec有什么优点、缺点？
Word2vec的模型参数怎么设置的？
余弦相似度？为什么使用余弦相似度？
Word2vec
Skip of gram
一个词预测上下文
窗口大小为n，则选取字上下个n个词。
将这个词和上下2n个词，分别组成样本，是正样本，预测处理
模型训练好后，则w即为这个词的词向量，
因为预测了2n次才得出模型，所以1个词的训练需要2n次，效率比cbow低，但是对生僻字的效果好
Cbow
上下文预测一个字
用上下文2n个词分别乘以权重w，在相加，进行预测，
模型训练好后，可一次性得出2n个词的词向量
训练效率高，但是准确度不如skip of gram
Negative Sampling
负采样，在skip of gram或者cbow中，最后预测都是多分类任务softmax，即预测出的结果是词表中的某一个词，这样分类较多，用softmax开销大，切不准确（每个词的概率太低），采用负采样后，是针对正负样本进行迭代（softmax只考虑正样本的loss），用交叉熵损失，而负样本的选取，依据概率来选，只更新正样本，和选中的部分负样本即可，完成这些样本label的词进行迭代w即可。大大提高效率


3.	FastText
fastText与CBOW的区别
fastText的模型和CBOW的模型结构一样，虽然结构一样，但是仍有不同
一、目的不一样，fastText是用来做文本分类的，虽然中间也会产生词向量，但词向量是一个副产物，而CBOW就是专门用来训练词向量的工具。
fastText的输出层是预测句子的类别标签，而CBOW的输出层是预测中间词；
fastText的输入层是一个句子的每个词以及句子的ngram特征，而CBOW的输入层只是中间词的上下文，与完整句子没有关系；
句子的n-gram特征，是按照句子的词，进行ngram组合，每个n-gram是一个向量，在吧句子中虽有的n-gram向量相加求平均，就是输入
fastText是一个文本分类算法，是一个有监督模型，有额外标注的标签
CBOW是一个训练词向量的算法，是一个无监督模型，没有额外的标签，其标准是语料本身，无需额外标注。

用fastText做文本分类的关键点是极大地提高了训练速度（在要分类的文本类别很多的情况下，比如500类），原因是在输出层采用了层级softmax，层级softmax如何提高训练速度在上面CBOW的层级softmax中已经介绍了，在这里就是将叶节点有词频变成文本分类数据集中每种类别的样本数量，霍夫曼树的结构也可以处理类别不均衡的问题（每种类别的样本数目不同），频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，这也使得进一步的计算效率更高（意思是数目多的样本深度小，那很多样本都只需乘一次就把概率计算出来了，自然就快）
4.Seq2seq,attention,self-attention,Transformer,bert
介绍下模型？
attention机制、self-attention机制的计算过程？为什么使用attention？它们的优点？
bert的结构和应用？bert的为什么这么厉害？

CNN→RNN→LSTM/GRU→seq2seq→seq2seq-Attention→Transformer-self Attention→bert
这是一条线，我在说RNN的部分时，就把这条线顺着说完。

NNLM→Word2vec→Doc2vec→FastText→ELMO→GPT→Bert
这是另一条线，被问到Word2vec时我先退回去说NNLM，然后在Word2vec，要是面试官不反感，又继续沿着线走。
seq2seq Attention
 
是针对seq2seq模型中的
Attention机制主要是在特征提取时，考虑每层输出结果，即是词之间的相关性
 
 
Attention机制，就是吧encoder的输出h1-ht，乘以系数，加到decoder的输入中，
则decoder的输入包含三个部分，
s（i-1）上一个递归状态
y（i-1）上一个输出
c（i） attention部分，其中，c（i）具体的计算如下
 是softmax进行归一化
 
 
seq2seq模型是基于lstm或者gru模型构建的，lstm和gru是用到了递归神经网络，即考虑上下文，前一时间t和后一时间的词关系，模型训练时，因为有时序性，不能进行并发计算，效率较慢
Lstm是提取得一个词和之前的所有词之间的关系，但是不是一个词和前面某一个词两两之间的关系

Transformer self-attention multihead
是对于transformer模型中的，特点是q，k，v相等，意思是qkv都是从相同的输入乘以不同的权重得来的，即，计算同一个输入内之间的关联性，不是qkv的向量值相等，而是输入相同，但w权重不同
self-attention不是递归模型，他的输入是整个的句子，不用等待上一个输入的输出作为下一个时刻的输入，这样可并行同时计算，但是因为参数要多很多，所以整体效率要慢10倍左右（准确率要比lstm高几个点1-3）
 
Multihead-attention
多头，就是有n个qkv的组，对每组qkv做操作，将结果进行拼接，多少个组就是多少个头（类似分组卷积）

https://baijiahao.baidu.com/s?id=1622064575970777188&wfr=spider&for=pc 
第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造
一个查询向量 q
一个键向量 k
一个值向量v
这三个向量是通过词嵌入与三个权重矩阵后相乘创建的
打分是通过本词的q和其他词的k相乘，得到其他词对本词的打分，再进行softmax归一化得到权重
权重再乘以v得到结果，实现self-attention
Multihead-attention多头就是多个q，k，v的组，注意多头后，是concat，并行拼接
 


Elmo，gpt，Bert区别
word2vec： nlp中最早的预训练模型，缺点是无法解决一词多义问题.
ELMO：
优点： 根据上下文动态调整word embedding,因为可以解决一词多义问题；
缺点：1、使用LSTM特征抽取方式而不是transformer，2、使用向量拼接方式融合上下文特征融合能力较弱。
GPT：.
优点：使用transformer提取特征
缺点：使用单项的语言模型，即单向transformer.
BERT： 优点：使用双向语言模型，即使用双向transformer；使用预测目标词和下一句这中多任务学习方式进行训练

4.1 Transformer
Transformer是谷歌在2017年6月论文 Attention is all you need发布的一个用来替代RNN和CNN的新的网络结构，Transformer本质上就是一个Attention结构，它能够直接获取全局的信息，而不像RNN需要逐步递归才能获得全局信息，也不像CNN只能获取局部信息，并且其能够进行并行运算，要比RNN快上很多倍。
https://blog.csdn.net/weixin_40005743/article/details/85460869
 
模型结构：
 
 
Self-attention的过程
单独拆开来看的话如下：
 
合起来计算
 
 
为什么除以dk
因为在softmax的分量增大时，最大那个值的softmax后的结果更接近1，此时在反向传播时，梯度很接近0，分量增大，就是每个数值增大，总体的方差也会增大，所以避免梯度消失，就是要将每个分量的值降低，就是取除以dk
Multi-head attention
自注意力机制的基础上又加了一个多头注意力机制，这个机制从两个方面增强了注意力层的表现：
(1)增加了模型将注意力集中在多个不同位置的能力
(2)muti-headed attention可以使我们拥有多组的 Query/Key/Value 权重矩阵（论文中是8组）。每一组都随机初始化经过训练之后，能够将输入的词向量转化为具有不同代表意义的子空间（different representation subspace）
多头之后，通过一个w0的矩阵将多个z压缩成1个
 
 
总结来说，self 是指输入的是相同的，即qkv的输入相同，再乘以不同的系数矩阵
Multihead是多头，指通过h个不同的线性变换对qkv进行头型，代表不同的子空间

整个流程合起来的：
 
位置编码
位置编码的定义有多种，其中一种是绝对位置编码（gpt、bert使用），还有相对位置编码（transformer原始论文使用）。
截止目前为止，我们介绍的Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。
为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。
那么怎么编码这个位置信息呢？常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。那么这个位置编码该是什么样子呢？通常位置编码是一个长度为d_modeldmodel的特征向量
 
位置编码演变
1．Pos=0，1，2，3，4，5…T
问题，
(1)	句子过长时，最后一个比第一个差值太大，和字嵌入合并后会有数值上的倾斜
(2)	句子过长，位置编码值过大时，会比字嵌入的数值大，影响字嵌入信息
2．Pos/T
问题
不同长度的文本，步长不同（相邻2个字的差值不同）
3．用sin/cos周期函数，sin(pos/a)  
问题：会有重复值
4. 用sin/cos周期函数，不同维度用不同函数
4.2 BERT
201810月11日，Google AI Language 发布了论文
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
Bert结构是在transformer模型的基础上（只用了左边的图），增加了句子之间的关系，和句子内词之间的关系来提取特征，进行词向量的训练，
采用多任务模式
1.2个句子是否是上下文，是为1，不是为0进行预测
2.	在句子中添加掩码，来预测这个掩码的词
掩码的添加规则
句子中15%的词会被掩码替代
1.15%的词中80%的概率被mask替代
2.15%的词中10%的概率不变
3.15%的词中10%的概率用其他词代替 
bert的输入数据通过3个信息相加
1.	词向量序列
2.	句子信息
3.	位置信息
为什么是相加，不是concat：因为每一次输入的位置信息都一样，如果吧位置信息不做相加处理，单独拿出来处理（concat），那么每一次的输入位置都相同，就没有意义
 


5.Seq2seq机器翻译和闲聊的区别

6.什么是语义理解/分析
依存句法分析的准确度是多少

7.信息抽取
8.ner中为什么用深度学习模型，而不是直接匹配标注好的词表呢？
直接匹配只能匹配词表中的词，而词表外的词不能匹配，深度学习可以一定程度上预测词表外的词
什么是新词发现？
专业性比较强，比较严谨的，很难做新词发现，因为没有新词了。新词发现是根据场景定的
Ner中在用bilstm模型和crf时，为什么不只用bilstm
只用bilstm时，如果句子是2个连续的名词，那么会识别错误，但是加上crf会考虑句子整体的信息，就不会分错
8.1.ner中为什么lstm+crf比lstm+softmax好？

条件随机场可以把label的上下文学出来。lstm加softmax分类的时候只能把特征的上下文关系学出来，label的没学出来。
简单理解，如果只是lstm+softmax那么会出现bb的label预测，而lstm+crf就不会出现bb的label预测，因为crf考虑的整个句子的上下文特征
因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。当然了，CRF还能学到更多的限定特征，那越多越好啊
9.常用工具
Hanlp/jieba
Gensim
Tensorflow
Pytorch 周莫烦
keras
10.深度文本匹配
求句子的相似度：
1.句子a和句子b，分词，bert求出词向量
2.求a中每个单词和b中每个单词的两两的相似度，得到相似度矩阵，再进行计算

文本相似度:
1.求2个文本中的主题词
2.求词的相似度矩阵

11.图谱知识点
RDF/RDFS/OWL和neo4j的关系
RDF/RDFS/OWL是本体语言，而本体的构成元组包含实例化数据，也就是说OWL不单单是概念的定义，还包括真实数据。
Neo4j是一个高性能的,NOSQL图形数据库，它将结构化数据存储在网络上而不是表中。Neo4j也可以被看作是一个高性能的图引擎，该引擎具有成熟数据库的所有特性。在一个图中包含两种基本的数据类型：Nodes（节点） 和 Relationships（关系）。Nodes 和 Relationships包含key/value形式的属性。Nodes通过Relationships所定义的关系相连起来，形成关系型网络结构
 
1.关系抽取
（1）模板匹配
（2）深度学习，将每个句子的关系类型标记成label，预测每个句子的关系类型，再将实体抽取出来，进行组合
（3）deepdive远程监督，是半监督，将 北京 首都两个关键词的句子，视为北京是首都的关系类型，凡是匹配到北京和首都词语的都是该类型的关系
2.知识推理
图谱中的隐含知识表示
例如 张三--儿子--张小三，那么此条关系就可以推出，张三已经结婚了或者领养（大概率）
此种推理可以写到图谱中，作为推理的结果
3.知识融合
从不同渠道获取的相同entity的内容
（1）内容相同，根据置信度将一个内容作为图谱的内容
（2）内容不同，那么将两个内容进行合并，并作为图谱的内容
12.对话系统
 
多伦对话中的指代消解：
1.用bert，前后句组合成1句，然后如果用后面的句子的embedding进行其他处理（后面句子的embedding就包含了指代消解的过程）
2.句法分析出指示代词的词性，比如说是地名，然后就拿之前的句子中的地名槽位进行指代消解
3.词性标注，将有指代词的句子进行词性标注，标注出示地名后，用之前句子中的地名槽位进行指代消解

13.rasa
1.police
policies：用于core模块，主要是根据意图和槽位，制定策略，预测next-action
也是分为rasa自带策略和用户自定义策略

Memoization Policy
如果训练数据中存在这样的对话，那么它将以置信度为1.0预测下一个动作，否则将预测为None，此时置信度为0.0
注：max_history值越大训练得到的模型就越大并且训练时间会变长

Keras Policy
是Keras框架中实现的神经网络来预测选择执行下一个action，它默认的框架使用LSTM(Long Short-Term Memory，长短期记忆网络)算法，但是我们也可以重写KerasPolicy.model_architecture函数来实现自己的框架(architecture)。KerasPolicy的模型很简单，只是单一的LSTM+Dense+softmax
注：epochs表示训练的次数，max_history同上

Embedding Policy
循环嵌入式对话策略(Recurrent Embedding DialoguePolicy，REDP)，它通过将actions和对话状态嵌入到相同的向量空间(vectorspace)能够获得较好的效果，
REDP包含一个基于改进的Neural Turing Machine的记忆组件和注意机制，在该任务上显著优于基线LSTM分类器

Form Policy
FormPolicy是MemoizationPolicy的扩展，用于处理(form)表单的填充事项。当一个FormAction被调用时，FormPolicy将持续预测表单动作，直到表单中的所有槽都被填满，然后再执行对应的FormAction

Mapping Policy
用于直接将意图映射到要执行的action，从而实现被映射的action总会被执行，其中，这种映射是通过triggers属性实现的
intents:
- greet: {triggers: utter_goodbye}
Fallback Policy
意图识别的置信度低于nlu_threshold，或者没有任何对话策略预测的action置信度高于core_threshold，FallbackPolicy将执行fallback action
policies:
 - name: "FallbackPolicy"
   # 意图理解置信度阈值
   nlu_threshold: 0.3
   # action预测置信度阈值
   core_threshold: 0.3
   # fallback action
   fallback_action_name: 'action_default_fallback'

14.	Tensorflow
15.	结巴分词的了解
结巴分词是用什么方法做的
官方Github上对所用算法的描述为：
基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；
采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；
对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。

16.	xlnet

2019年 6 月，CMU 与谷歌大脑提出的 XLNet 在 20 个任务上超过 BERT，并在 18 个任务上取得当前最佳效果的表现。
XLNet 是一个类似 BERT 的模型，而不是完全不同的模型。但这是一个非常有前途和潜力的。总之，XLNet是一种通用的自回归预训练方法
那么什么是自回归（AR）语言模型？AR语言模型是一种使用上下文词来预测下一个词的模型。但是在这里，上下文单词被限制在两个方向，前向或后向。
自回归语言模型有优点有缺点，缺点是只能利用上文或者下文的信息，不能同时利用上文和下文的信息
XLNet和BERT有什么区别？
与 AR 语言模型不同，BERT 被归类为自动编码器（AE）语言模型。
AE 语言模型旨在从损坏的输入重建原始数据
AE 语言模型的优势是，它可以从向前和向后的方向看到上下文
17.	Albert
参数量小的bert
18.	Drop的作用，在tf，keras，pytorch的用法，以及测试训练集的不同
作用，增加泛化，防止过拟合。减弱模型能力，达到降低过拟合的作用（具体来说，就是在某一次迭代中，随机的使一些参数项为0，不参与迭代过程和计算结果，减弱参数间的关联性）
通常设置值在0.2-0.5之间，只在训练时有dropout，在测试时没有dropout但是需要按照dropout比率缩小，加以平衡


19.	Normalization
标准化，也是泛化，防止过拟合，有助于梯度传播，特别时对于深层网络，加多个bn才能训练
通常在卷积层或密集的连接层之后
测试时，每个batch的均值的均值，方差是每个batch的无偏估计量，或者滑动平均值保存，用于测试，测试时，这个值是固定的
分为：BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm
其中LayerNormalization多用于rnn
20.	Focal Loss
在交叉熵损失，加入a（1-p）**g  g是平滑因子gamma,a是系数alpha
调制因子能够减少 easy 样本对于损失函数的贡献，并延伸了loss 值比较地的样本范围.

